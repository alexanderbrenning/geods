---
title: "How to encode slope aspect (and other cyclic variables) in environmental ML models"
date: '2021-12-29'
slug: directional-variables-in-ml-models
categories: ["R", "machine learning", "environmental modeling", "feature engineering"]
tags: ["geomorphometry", "slope aspect", "directional variables", "binning", "discretization", "encoding", "landslide susceptibility modeling", "species distribution modeling", "permafrost distribution modeling", "digital soil mapping", "model visualization", "interpretable machine learning", "iml", "ALE plot", "partial dependence plot", "permutation variable importance", "random forest", "support vector machine", "generalized additive model", "logistic regression"]
summary: 'Slope aspect is a predictor in many environmental machine-learning models. Here I compare strategies for dealing with problems caused by the cyclic nature of this directional variable.'
lastmod: '2021-12-29'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
bibliography: encoding-slope-aspect.bib
csl: chicago-author-date.csl
draft: false
---

```{r setup, include=FALSE}
library("knitr")
library("kableExtra")
knitr::opts_chunk$set(echo = FALSE)
```


```{r prep, include=FALSE}
library("ggplot2")
library("patchwork")
library("purrr")

RES <- readRDS("R/aspect-results.rds")
res <- RES$res # for convenience

sel <- which(is.na(res$Aspect))
res$Aspect[sel] <- 0
res[ sel - 1, ] <- NA
```

Slope aspect is a standard ingredient of statistical and machine-learning models of Earth surface processes and topoclimatic variables, and in digital soil mapping and ecological modeling [e.g., @brenning.et.al.2015.highways;@deluigi.et.al.2017.permafrost].
But unlike other quantitative predictors, it's a strange beast:

- The distance between 359° and 1° is 2°, not 358°.
- Is 180° greater or smaller than 0° (=360°), or can it be both?
- Two times 180° is 0° (or at least equivalent to 0°).

Unfortunately, all statistical and ML models apply at least one of these concepts - distance (or subtraction), order, and multiplication (and addition) to their predictor variables, including slope aspect.

You'll also realize that the same thing happens with other cyclic and directional variables such as the time of the day [@london.2016.cyclic], or the strike and dip of geological strata.

{{% callout warning %}}

So, do we have a problem?

{{% /callout %}}

Yes, we do. Or we might. Let's take a closer look at it to see how bad it might get. We'll apply four statistical and ML techniques to a simulated dataset to compare the following strategies:

- **Direct encoding**: Slope aspect as an ordinary, untransformed predictor [@meyer.et.al.2019];
- **Binning**: Slope aspect is discretized into (usually eight) sectors and treated as a categorical variable; this is the most popular approach in landslide modeling;
- **Cosine-sine encoding**: Cosine and sine of aspect as predictors, making use of the fact that angles can be uniquely represented by their cosine and sine values [@stage.1976.aspect;@brenning.trombotto.2006.logreg;@brenning.et.al.2015.highways];
- And in the GAM, a **cyclic smoothing spline** [@wood.2017.book].


### A simulated toy dataset

My simulated data (`r paste0("$N=", nrow(RES[["data"]]), "$")`) mimics a situation that we might encounter when modeling Earth surface features that depend on ground temperature, such as permafrost or species distribution and characteristics [@boeckli.et.al.2012.method;@stage.salas.2007]. 
I made up a topographically controlled temperature variable, added some noise, and created presence/absence observations based on temperatures being greater than a given threshold, e.g. 0°C for permafrost.

Here, topographic control relates to elevation (lapse rate), slope aspect, and, to a smaller extent, slope angle and (logarithm of) upslope contributing area. 
Slope aspect relates mainly to solar radiation (north versus south exposure), but also to indirect effects such as snow cover onset in fall. Early snow cover "warms" the ground due to thermal insulation.
In my simulated world, snow cover builds up earlier in north-northwest direction (335°, where 0° is north and angles are in clockwise direction). 
I'll stop here, the other effects are minor.

So overall, permafrost should be more likely in the northeasterly direction, and the directional relationship is not perfectly symmetric.

### GAM and logistic regression

Let's start by looking at the generalized additive model (GAM) and the generalized linear model (GLM) or logistic regression since these are very popular environmental modeling techniques.
The GAM is an extension of the GLM in which each predictor variable is nonlinearly transformed by a smoothing spline [@wood.2017.book].
Categorical variables (such as binned aspect) can be represented using indicator variables, i.e. variables that have a value of 1 within a given aspect interval and 0 otherwise.

Due to the additivity of GAM and GLM models, we can determine how much a specific value of slope aspect contributes to the prediction of the response, or more precisely, the logit of the probability of presence.

We want to focus our attention on model behaviour at a slope aspect of 0°, which is equivalent to 360°. Instead of plotting the x axis from 0° to 360°, I therefore shifted it by 180°, i.e. we look at directions from -180° to +180°, or 180° ... 360° (= 0°) ... 180°. The grey curve represents the true simulated relationship.

```{r gamglm, echo=FALSE, fig.height=4, fig.width=8}
par(mfrow = c(1,2))
par(cex = 0.7, cex.main = 1.2)
ylim <- c(min(c(res$glm.direct,res$gam.cyclic,res$svm.cos_sin), na.rm = TRUE),
          max(c(res$glm.direct,res$gam.cyclic,res$svm.cos_sin), na.rm = TRUE))
ylim <- ylim * c(1.1, 1.35)
xat <- seq(-180, 180, by = 45)
xlim <- c(-180, 180)
xlabels <- as.character(xat %% 360)
xlabels[xlabels == "0"] <- "0 = 360"
plot(res$Aspect, res$gam.direct, type = "l", col = "red", lwd = 2, 
     xlim = xlim, ylim = ylim,
     xlab = "aspect [°]", ylab = "marginal effect on logit",
     main = "Generalized additive model",
     xaxs = "i", yaxs = "i", xaxt = "n")
axis(1, at = xat, labels = xlabels)
lines(res$Aspect, res$gam.binning, col = "darkgreen", lwd = 2)
lines(res$Aspect, res$gam.cyclic, col = "black", lwd = 2)
lines(res$Aspect, res$gam.cos_sin, col = "blue", lwd = 2)
lines(res$Aspect, res$truth, col = "grey50")
abline(v = 0, col = "grey80")
legend(x = -180, y = ylim[2], 
       lwd = c(1,2,2,2,2), 
       col = c("grey50", "red", "darkgreen", "blue", "black"), 
       bty = "n",
       legend = c("truth", "direct encoding", "binning", "cosine-sine encoding", "cyclic smoother"))

plot(res$Aspect, res$glm.direct, type = "l", col = "red", lwd = 2, 
     xlim = xlim, ylim = ylim,
     xlab = "aspect [°]", ylab = "marginal effect on logit",
     main = "Logistic regression",
     xaxs = "i", yaxs = "i", xaxt = "n")
axis(1, at = xat, labels = xlabels)
lines(res$Aspect, res$glm.binning, col = "darkgreen", lwd = 2)
lines(res$Aspect, res$glm.cos_sin, col = "blue", lwd = 2)
lines(res$Aspect, res$truth, col = "grey50")
abline(v = 0, col = "grey80")
```

It turns out that the direct encoding results in a discontinuity at 0° in the effect of aspect on the predictions.
The gap's magnitude is about 2 on the logit scale, which means that the odds ($p/(1-p)$) change by a factor of about `r paste0("exp(2) = ", exams::fmt(exp(2), digits = 1))`.
This is not plausible at all, and it would result in erratic patterns in prediction maps.

{{% callout warning %}}

Clearly, directional variables need to be treated differently.

{{% /callout %}}

In the **GAM**, the cyclic smoother and the cosine-sine encoding both produce reasonable approximations of the true curve, considering uncertainties due to random variability.
Nevertheless, with cosine-sine encoding, the secondary peak is slightly shifted, which is a phenomenon I frequently observed in different simulation runs.
Although the cyclic smoother does not decompose slope aspect into its cosine and sine, it is capable of modeling a nonlinear relationship that is continuous everywhere in the cycle.

{{% callout note %}}

Cyclic smoothers are a great feature of GAMs, but cosine-sine encoding also works.

{{% /callout %}}

In **logistic regression**, direct encoding also produces a gap, and the modeled relationship is not even strictly linear, as it has a breakpoint at 0° in our shifted plot.
The GLM with cosine-sine encoding is unable to model the two peaks; this may not be a problem when datasets are small or slope aspect has little importance, but in general we may find this too limiting.

Binning the aspect variable results in a step function with eight levels.
The effects associated with each aspect interval are only supported by the number of observations within that interval, which can be a limiting factor in many practical applications (but not in my simulated case study).
While binned variables are easy to interpret, the step function results in implausible discontinuities in prediction maps.

The **degrees of freedom** associated with slope aspect are a good summary of how much flexibility the models deem necessary when modeling this relationship using different strategies:


```{r edftable, echo=FALSE}
EDF <- data.frame(
  Model = c(rep("GLM", 3), rep("GAM", 4)),
  Strategy = c(
    rep(c("Direct encoding", "Binning", 
          "Cosine-sine encoding"), 2), 
    "Cyclic smoother"),
  "Degrees of freedom" = c("1","7","2",
                           exams::fmt(RES$edf$direct), 7,
                           exams::fmt(RES$edf$cos_sin),
                           exams::fmt(RES$edf$cyclic))
)

cnms <- c("Model", "Strategy", "Degrees of freedom")

EDF %>% kableExtra::kbl(align = "llr", col.names = cnms) %>% 
  kableExtra::kable_styling(full_width = FALSE)

```

### What about more flexible ML techniques?

Next, I use the support vector machine (SVM) and random forest (RF) classifiers with direct and cosine-sine as well as binned encodings of slope aspect.
Both models were tuned using 5-fold cross-validation; random forest uses `r RES$fit$rf$ntree` trees to reduce artefacts.

Partial dependence plots display the marginal effects of predictors on the response in these non-additive models:

```{r svmrf, echo=FALSE, fig.height=4, fig.width=8}
par(mfrow = c(1,2))
par(cex = 0.7, cex.main = 1.2)
plot(res$Aspect, res$svm.direct, type = "l", col = "red", lwd = 2, 
     xlim = xlim, ylim = ylim,
     xlab = "aspect [°]", ylab = "marginal effect on logit",
     main = "Support vector machine",
     xaxs = "i", yaxs = "i", xaxt = "n")
axis(1, at = xat, labels = xlabels)
lines(res$Aspect, res$svm.binning, col = "darkgreen", lwd = 2)
lines(res$Aspect, res$svm.cos_sin, col = "blue", lwd = 2)
lines(res$Aspect, res$truth, col = "grey50")
abline(v = 0, col = "grey80")
legend(x = -180, y = ylim[2], 
       lwd = c(1,2,2,2), 
       col = c("grey50", "red", "darkgreen", "blue"), 
       bty = "n",
       legend = c("truth", "direct encoding", "binning", "cosine-sine encoding"))

plot(res$Aspect, res$rf.direct, type = "l", col = "red", lwd = 2, 
     xlim = xlim, ylim = ylim,
     xlab = "aspect [°]", ylab = "marginal effect on logit",
     main = "Random forest",
     xaxs = "i", yaxs = "i", xaxt = "n")
axis(1, at = xat, labels = xlabels)
lines(res$Aspect, res$rf.binning, col = "darkgreen", lwd = 2)
lines(res$Aspect, res$rf.cos_sin, col = "blue", lwd = 2)
lines(res$Aspect, res$truth, col = "grey50")
abline(v = 0, col = "grey80")
```

SVM exhibits the same problems we saw in logistic regression, but the cosine-sine encoding is certainly the better choice.
Model shrinkage is very strong here, and therefore the relationships are strongly simplified.

RF shows the typical behaviour with fine step-like patterns and some artefacts in spite of the large number of trees used (2000).
Focusing on the critical point at 0° = 360° aspect, there's a relatively small jump in the RF with direct encoding of aspect, and no offset when using cosine-sine encoding.
Nevertheless, the jump is not particularly remarkable compared to the usual variability in RF marginal effects.

Binning shows the expected step-function behaviour, which does not exploit the capabilities of SVM and RF to model more complex nonlinear relationships.

Overall, some ML models may be more sensitive to the peculiarities of directional variables, and we'd expect smooth models like SVM or artificial neural networks to benefit most strongly from cosine-sine encodings than tree-based models including boosting.

{{% callout note %}}

Cosine-sine encoding is the best general solution for handling directional variables in machine-learning models.

{{% /callout %}}

### How can I obtain model diagnostics for aspect when using cosine-sine encoding?

If you are familiar with model diagnostics such as ALE plots and permutation-based variable importance (PVI) [e.g., @molnar.2019.iml.book], you may wonder how these tools can be applied to visualize the overall importance of aspect -- and not of its cosine and sine separately.
The problem is that cosine and sine do not vary independently of each other.
For example, a cosine of 1 can only be combined with a sine of 0 in order to obtain a valid point on the unit circle representing an angle in polar coordinates.

{{% callout warning %}}

Always look at the *joint* effect of the cosine and sine of directional variables.

{{% /callout %}}

We can solve this by looking at the encoding-based models from the perspective of slope aspect itself.
We just need a wrapper function for the predict method that calculates the encoded features based on the input aspect values.
This wrapper function can be input into the function of the [`iml` package](https://github.com/christophM/iml) for creating ALE plots and calculating permutation-based variable importance [@molnar.et.al.2018.iml].
By the way, this is a special case of the transformation approach I [proposed recently](https://geods.netlify.app/post/interpretable-ml-with-a-twist/) to examine models in terms of features that aren't even included in the model [@brenning.2021.wiml].

The following ALE plots and variable importances examine the marginal relationship between aspect and the response regardless of the chosen encoding:


```{r svmwiml, echo=FALSE, fig.height=5, fig.width=7, warning=FALSE}
theme_set(theme_grey(base_size = 10))
cols <- c(
  Direct = "red",
  Binning = "darkgreen",
  "Cosine-sine" = "blue"
)
ale <- ggplot(RES$ale[RES$ale$model == "svm",], 
              aes(x = Aspect, y = ALE, group = Encoding, colour = Encoding)) + 
  scale_color_manual(values = cols) +
  geom_line(size = 1) +
  ggtitle("ALE plots of slope aspect in SVM models with different encodings")
imp_direct <- ggplot(RES$imp$svm.direct, aes(y = feature, x = importance)) +
  geom_col(fill = "red") + ggtitle("Direct encoding")
imp_binning <- ggplot(RES$imp$svm.binning, aes(y = feature, x = importance)) +
  geom_col(fill = "darkgreen") + ggtitle("Binning")
imp_cos_sin <- ggplot(RES$imp$svm.cos_sin, aes(y = feature, x = importance)) +
  geom_col(fill = "blue") + ggtitle("Cosine-sine encoding")
ale / (imp_direct | imp_binning | imp_cos_sin)

```


## Take-home message

So, what have learned?

{{% callout note %}}

- Never use slope aspect (and other cyclic variables) as a predictor without further ado.
- In a GAM, cyclic smoothers are the best way to include slope aspect.
- In general machine-learning models, use cosine-sine encoding to avoid artifacts.
- Avoid binning aspect; cosine-sine encoding and cyclic smoothers provide more flexibility with fewer degrees of freedom.
- If there's a specific azimuth direction (or point in the cycle) where you expect the maximum or minimum, you can shift the aspect variable before calculating the (co)sine.

{{% /callout %}}

### R Code

The R code for this analysis is available in [Github](https://github.com/alexanderbrenning/geods/tree/main/content/post/directional-variables-in-ml-models/R).

### References

<div id="refs"></div>

<img src="https://vg09.met.vgwort.de/na/c808060af5344007b8c22dfaa54d3117" width="1" height="1" alt="">
