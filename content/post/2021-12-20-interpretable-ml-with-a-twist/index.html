---
title: "The wiml Package: Transforming Feature Space to Interpret Machine Learning Models"
date: '2021-12-20'
slug: interpretable-ml-with-a-twist
categories: ["R", "machine learning", "remote sensing"]
tags: ["model visualization", "interpretable machine learning", "R package"]
subtitle: 'A Remote-Sensing Case Study Using the Random Forest Classifier'
summary: 'Main effects plots (such as partial dependence and ALE plots) can be confusing and even misleading when interpreting black-box machine-learning models with large numbers of highly correlated features, as in remotely-sensed land cover classification. The `wiml` package implements a simple and pragmatic approach to dealing with this problem. This approach can be especially beneficial in situations where features tend to be linearly dependent, or in other words, where principal components analysis seems like a reasonable approach. This post is a package vignette that walks you through an application of the proposed approach in remote sensing.'
lastmod: '2021-12-20T01:10:51+01:00'
featured: no
image:
  caption: ''
  focal_point: ''
  preview_only: no
projects: []
bibliography: wiml.bib
csl: chicago-author-date.csl
---

<script src="{{< blogdown/postref >}}index_files/header-attrs/header-attrs.js"></script>


<p>Main effects plots (such as partial dependence and ALE plots) can be confusing and even misleading when dealing with large numbers of highly correlated features. Example applications include land cover classification using multitemporal satellite remote-sensing data or texture features derived from such imagery. This package introduces a simple and pragmatic approach to dealing with this problem. This approach can be especially beneficial in situations where features tend to be linearly dependent, or in other words, where principal components analysis seems like a reasonable approach.</p>
<div id="preface" class="section level2">
<h2>Preface</h2>
<p>This vignette walks you through most of the analyses performed for the paper that introduces the novel model interpretation approach implemented in the <code>wiml</code> package. Please refer to that paper for conceptual and formal details, and cite it when using <code>wiml</code> or referring to the methods and results presented herein.</p>
<blockquote>
<p><span class="citation"><a href="#ref-brenning.2021.wiml" role="doc-biblioref">Brenning</a> (<a href="#ref-brenning.2021.wiml" role="doc-biblioref">2021</a>)</span> Transforming Feature Space to Interpret Machine Learning Models. arXiv preprint, arXiv:2104.04295, <a href="https://arxiv.org/abs/2104.04295" class="uri">https://arxiv.org/abs/2104.04295</a></p>
</blockquote>
<p>The <code>wiml</code> package serves as a thin wrapper around packages implementing ALE plots, partial dependence plots and other post-hoc model-agnostic interpretation tools. In this vignette, I will use <code>iml</code>; creating a vignette for use with <code>DALEX</code> is on my to-do list.</p>
<p>For an introduction to interpretable machine learning, see <span class="citation"><a href="#ref-molnar.2019.iml.book" role="doc-biblioref">Molnar</a> (<a href="#ref-molnar.2019.iml.book" role="doc-biblioref">2019</a>)</span>, and for a broader overview, <span class="citation"><a href="#ref-murdoch.et.al.2019.iml" role="doc-biblioref">Murdoch et al.</a> (<a href="#ref-murdoch.et.al.2019.iml" role="doc-biblioref">2019</a>)</span>. We are specifically dealing with the situation of post-hoc model-agnostic dataset-level tools for the interpretation of black-box machine-learning models.
Several of the following steps are computationally expensive and will be slow even on a workstation since the <code>iml</code> package, which does all the heavy lifting, does not seem to make full use of the parallel <em>workers</em> offered to it, at least not at the time of writing this document.</p>
</div>
<div id="getting-started" class="section level2">
<h2>Getting started</h2>
<div id="work-environment" class="section level3">
<h3>Work environment</h3>
<p>Make sure that all required packages and their dependencies are installed and up-to-date. <code>wiml</code> is currently only available via Github, so you will have to use <code>devtools</code> to install it; I also recommend using the most recent development version of <code>iml</code>.</p>
<p>In addition, you will need the packages <code>stringr</code>, <code>purrr</code>, <code>ggcorrplot</code>, <code>ggfortify</code>. Packages for parallelization are optional: <code>future</code>, <code>future.callr</code>. To parallelize computations, you can (optionally) use the following call, specifying the number of workers appropriate for your computing environment:</p>
<pre class="r"><code>library(&quot;future&quot;)
library(&quot;future.callr&quot;)
future::plan(&quot;callr&quot;, workers = 4)
set.seed(444)
options(future.rng.onMisuse = &quot;ignore&quot;)</code></pre>
</div>
<div id="case-study-and-data-preparation" class="section level3">
<h3>Case study and data preparation</h3>
<p>Land cover classification is a standard task in remote sensing, which often uses a large set of features (<span class="math inline">\(20\le p\le200\)</span>) - for example, multitemporal spectral reflectances and derived vegetation indices and texture attributes, or even hyperspectral features. Many of these features are strongly correlated with each other, and they are often semantically grouped.</p>
<p>We will look at a rather challenging case study on the detection of rock glaciers in the Chilean Andes using the random forest classifier and a combination of 40 texture features and 6 terrain attributes. This is how a rock glacier looks like - it resembles a lava stream more than a glacier:</p>
<div class="figure" style="text-align: center"><span style="display:block;" id="fig:unnamed-chunk-2"></span>
<img src="figures/rockglacier.jpg" alt="A rock glacier in the Andes of Santiago, Chile. (c) A. Brenning, CC BY-SA 4.0." width="60%" />
<p class="caption">
Figure 1: A rock glacier in the Andes of Santiago, Chile. (c) A. Brenning, CC BY-SA 4.0.
</p>
</div>
<p>The texture features were generated by applying Gabor filters with varying bandwidth, anisotropy and aggregation settings to an IKONOS satellite image, resulting in strongly correlated features. This case study is described in more detail by <span class="citation"><a href="#ref-brenning.et.al.2012.gabor" role="doc-biblioref">Brenning, Long, and Fieguth</a> (<a href="#ref-brenning.et.al.2012.gabor" role="doc-biblioref">2012</a>)</span>. We use a sample of 1000 points from this data set’s Laguna Negra area (500 presence and 500 absence locations).</p>
<p>Let’s get started by preparing the data set, which is shipped with the <code>wiml</code> package:</p>
<pre class="r"><code>data(gabor, package = &quot;wiml&quot;)
d &lt;- gabor[gabor$area == &quot;LN&quot;, ]
colnames(d) &lt;- gsub(&quot;m30&quot;, &quot;&quot;, colnames(d))

sel &lt;- c(sample(which(d$class == &quot;FALSE&quot;), size = 500),
         sample(which(d$class == &quot;TRUE&quot;), size = 500))
d &lt;- d[sel,]

# Set up lists of features and model formulas:
gabor_vars &lt;- stringr::str_subset(colnames(d), &quot;e[12]g[1-5]&quot;)
terrain_vars &lt;- c(&quot;dem&quot;, &quot;slope&quot;, &quot;pisr&quot;, 
                  &quot;cslope&quot;, &quot;log.carea&quot;, &quot;log.cheight&quot;)
yvar &lt;- &quot;class&quot;
Xvars &lt;- c(gabor_vars, terrain_vars)

# Formula for fitting the model with all features:
fo &lt;- as.formula(paste(yvar, &quot;~&quot;, 
                       paste(Xvars, collapse = &quot; + &quot;)))

# Trim the tails of distributions to remove outliers:
d[, Xvars] &lt;- d[, Xvars] %&gt;% 
  purrr::map(DescTools::Winsorize, probs = c(0.02, 0.98)) %&gt;%
  as.data.frame()
# Scale only Gabor features:
d[, gabor_vars] &lt;- scale(d[, gabor_vars])</code></pre>
</div>
<div id="exploratory-analysis" class="section level3">
<h3>Exploratory analysis</h3>
<p>(Impatient readers: please skip this section.)</p>
<p>To show you how strongly the features are correlated, let’s take a look at this correlation matrix; note that the terrain attribute are the six features at the top:</p>
<pre class="r"><code>ggcorrplot::ggcorrplot(cor(d[, Xvars]), type = &quot;upper&quot;) +
  ggplot2::theme_grey(base_size = 8)</code></pre>
<p><img src="figurescorrplot-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>To wrap our head around the information contained in the texture features, we take a look at a principal components analysis (PCA) - the following plot shows the weights of the (standardized) texture features in the first four principal components, which explain about 85% of the variance:</p>
<pre class="r"><code>library(&quot;ggfortify&quot;)
ggplot2::autoplot(prcomp(d[, gabor_vars])$rotation[,1:4]) +
  scale_fill_gradient2(low = &quot;blue&quot;, high = &quot;red&quot;, mid = &quot;white&quot;, 
   midpoint = 0, limit = c(-.4,.4), space = &quot;Lab&quot;, 
   name=&quot;Weight&quot;) +
  ggplot2::theme_grey(base_size = 9)</code></pre>
<p><img src="figuresstrucpcaplot2-1.png" width="50%" style="display: block; margin: auto;" /></p>
</div>
</div>
<div id="post-hoc-interpretation-the-traditional-way" class="section level2">
<h2>Post-hoc interpretation, the traditional way</h2>
<p>Before we apply the feature space transformation I propose in the paper, let’s start with a traditional post-hoc interpretation at the level of the original features. We will train a random forest model on our 46 features, and create accumulated local effects (ALE) plots as our preferred way of visualizing the main effects of features in our black-box model <span class="citation">(<a href="#ref-molnar.2019.iml.book" role="doc-biblioref">Molnar 2019</a>)</span>:</p>
<pre class="r"><code>fit &lt;- randomForest::randomForest(formula = fo, data = d, importance = TRUE)</code></pre>
<pre class="r"><code>simple_predictor &lt;- Predictor$new(fit, data = d, y = yvar, 
                                  type = &quot;prob&quot;, class = &quot;TRUE&quot;)
simple_effs &lt;- FeatureEffects$new(simple_predictor, 
                                  features = Xvars, 
                                  method = &quot;ale&quot;)</code></pre>
<p>Well that took some time… here’s finally our figure:</p>
<pre class="r"><code>plot(simple_effs, ncols = 8)</code></pre>
<p><img src="figuresplotale-1.png" width="100%" style="display: block; margin: auto;" /></p>
<p>Can you find a pattern? You’re probably totally lost because you don’t know what the texture feature names mean; but trust me, it doesn’t get much better if you know their meanings. How about showing this figure to a conference audience? Better don’t try…</p>
<p>The same thing happens if we look at permutation-based variable importances, which are also problematic due to the strong correlations. We’ll take a quick look using <code>randomForest</code>’s built-in method:</p>
<pre class="r"><code>randomForest::varImpPlot(fit, n.var = 20, type = 1)</code></pre>
<p><img src="figuresvarimpplot-1.png" width="60%" style="display: block; margin: auto;" /></p>
</div>
<div id="model-interpretation-in-transformed-feature-space" class="section level2">
<h2>Model interpretation in transformed feature space</h2>
<p>Now you may remember that many of the texture features are strongly correlated, and according to our domain knowledge, they are semantically related to each other as the features correspond to isotropic or anisotropic Gabor filters, wavelengths from 5 to 50 m, and four different aggregation schemes. We’d really like to think about decomposing their variance into… principal components!</p>
<p>The relationships between the texture features are in fact reasonably close to linear, and principal component analysis (PCA) is therefore an obvious choice for feature engineering. We may be tempted to retrain our model using the principal components (PCs) instead of the original features. Nevertheless, there are two reasons why I’m going down that road:</p>
<ol style="list-style-type: decimal">
<li>Feature engineering, and PCA in particular, does not always improve model performance; it may in fact be harmful. (Spoiler: It’s not harmful in this case study.)</li>
<li>More importantly, in post-hoc analyses, we are not normally able or allowed to retrain the model. The whole point of post-hoc methods is that they can be applied to trained models.</li>
</ol>
<p>But how can we examine our model’s behaviour from a PCA perspective without retraining the model?</p>
<p>Here’s the trick: We’ll use an invertible transformation function as a pair of glasses that’ll allow us to inspect the model’s behaviour. The transformation function we’ll be using is a principal component transformation.</p>
<p>I’ll briefly outline the mathematical formalism, in case you’re interested. We’re dealing with a model
<span class="math display">\[
\hat{f}:\mathbf{x}\in\mathbb{R}^p\mapsto\hat{f}(\mathbf{x})\in\mathbb{R}
\]</span>
that was fitted to a training sample <span class="math inline">\(L\)</span> in the (original, untransformed) <span class="math inline">\(p\)</span>-dimensional feature space <span class="math inline">\(X\subset\mathbb{R}^p\)</span>, where the predicted values are probabilities or logits, in the case of classifiers. We’ll use an invertible transformation function
<span class="math display">\[
\mathbf{T}: X \rightarrow W\subset\mathbb{R}^p,\quad \mathbf{w} = \mathbf{T}(\mathbf{x})
\]</span>
that re-expresses the features in our dataset in a way that makes sense to us as domain scientists. PCA seems reasonable here, but other transformations can be considered (some thoughts on this in the paper).</p>
<p>The composition of the back transformation <span class="math inline">\(\mathbf{T}^{-1}\)</span> and the trained model function <span class="math inline">\(\hat{f}\)</span> now formally defines a model <span class="math inline">\(\hat{g}\)</span> on <span class="math inline">\(W\)</span>,
<span class="math display">\[
\hat{g} := \hat{f}\circ\mathbf{T}^{-1}
\]</span>
which predicts the real-valued response based on ‘data’ in <span class="math inline">\(W\)</span> although it was trained using a learning sample <span class="math inline">\(L\subset X\)</span> in the untransformed feature space. So <span class="math inline">\(\mathbf{T}^{-1}\)</span> is the ‘thin layer’ I was talking about - a set of glasses, or maybe in this case we should call it a ‘macroscope’ as we’re aggregating information into principal components. The function <span class="math inline">\(\hat{g}\)</span> behaves exactly like a model trained on <span class="math inline">\(\mathbf{T}(L)\)</span>, both mathematically and computationally, and therefore we can fool any interpretation tool or R package by applying them to <span class="math inline">\(\hat{g}\)</span> instead of <span class="math inline">\(\hat{f}\)</span>.</p>
<p>Note that the PCs are linearly independent, and therefore we also overcome the issue we had with permutation variable importances calculated from strongly correlated features.</p>
<p>Before we continue, there’s still one pitfall to avoid: If we apply PCA to all features, we will ‘mix’ information from the texture and terrain features, although they are weakly correlated and semantically completely different. We will therefore use a transformation that does PCA on the texture features, and nothing (i.e. an identity transformation) on the terrain attributes. (This is achieved by creating a rotation matrix that is composed of two block matrices.)</p>
<p>By the way, <code>wiml</code> also allows us to perform (truely) structured PCAs, i.e. seperate PCAs on subsets of features. This would be useful here if the terrain attributes were strongly correlated, which is not the case.</p>
</div>
<div id="interpretation-using-structured-pca" class="section level2">
<h2>Interpretation using structured PCA</h2>
<p>So let’s define a structured transformation function <span class="math inline">\(\mathbf{T}\)</span> that PCA-transforms the 40 texture feature, but leaves the terrain attributes untouched.</p>
<p>The <code>wiml</code> package refers to transformation functions as <em>warpers</em>, just because it sounds cool.</p>
<pre class="r"><code># List for structured PCA:
wrp &lt;- pca_warper(d, xvars = gabor_vars, yvar = yvar, 
                  uvars = terrain_vars, wvars = &quot;Gabor&quot;)</code></pre>
<p>This is an object of class <code>warper</code>, which in this case is built around a <code>prcomp</code> object. Its <code>plot</code> method provides some insights into the PCA - we’ve seen enough of that in the exploratory analysis.</p>
<p>So now we’ve got our transformation function <span class="math inline">\(\mathbf{T}\)</span>, which can be applied to a dataset by using the <code>warp</code> method, and whose inverse is given by the inverse of its rotation matrix and implemented in the <code>unwarp</code> method.</p>
<p>We now have to wrap our transformation around the trained model, i.e. create the composition <span class="math inline">\(\hat{f}\circ\mathbf{T}^{-1}\)</span>:</p>
<pre class="r"><code>wfit &lt;- warp_fitted_model(fit, warper = wrp)</code></pre>
<p>This doesn’t really ‘do’ anything - it simply creates an object of class <code>warped_model</code>, which is equipped with a <code>predict</code> method. This method accepts inputs in transformed feature space, i.e. texture PCs and (unchanged) terrain attributes, and then feeds our trained model <code>fit</code> (or <span class="math inline">\(\hat{f}\)</span>) with back tranformed features in the original feature space. Let’s try that out:</p>
<div id="permutation-variable-importance" class="section level3">
<h3>Permutation variable importance</h3>
<p>As you can see, our small sample of four points actually consisted of PC data (features <code>Gabor1</code> etc.), and <code>wfit</code> behaves like a model that was fitted on PC-transformed data.</p>
<p>This means that we’re ready to fool <em>any</em> model-agnostic interpretation tool - just don’t tell’em that ‘wfit’ is not a real model!</p>
</div>
<div id="permutation-variable-importance-1" class="section level3">
<h3>Permutation variable importance</h3>
<p>We’ll start with the permutation variable importance:</p>
<pre class="r"><code>imp_wpredictor &lt;- Predictor$new(wfit, 
                                data = warp(d, warper = wrp), 
                                y = yvar, type = &quot;response&quot;)
imp &lt;- FeatureImp$new(imp_wpredictor, 
                      loss = &quot;ce&quot;, compare = &quot;difference&quot;, 
                      n.repetitions = 50)</code></pre>
<p>I’ll spare you this messy plot, let’s focus on the top 10 features (At the time of writing this document, the <code>features</code> argument was only available in the development version of <code>iml</code>):</p>
<pre class="r"><code>imp10 &lt;- FeatureImp$new(imp_wpredictor, 
                      loss = &quot;ce&quot;, compare = &quot;difference&quot;,
                      features = imp$results$feature[1:10],
                      n.repetitions = 50)</code></pre>
<pre class="r"><code>ggplot2::theme_set(theme_grey(base_size = 12)) # back to normal
plot(imp10)</code></pre>
<p><img src="figuresstrucpcaimp10plot-1.png" width="50%" style="display: block; margin: auto;" /></p>
<p>So, the first two principal components of Gabor texture features <em>are</em> indeed important. <code>Gabor1</code> is essentially the mean value across all texture features - an overall measure of stripedness - which underlines that we’re doing the right thing when applying Gabor filters. This, and the information we gain from PCs #2 and #3, weren’t visible at all in the original variable importance plot above.</p>
<p>Also note that permutation on the untransformed feature resulting in a lot of extrapolation beyond the data-supported region due to the strong correlations between features. This is not the case here any more - the PCs are uncorrelated!</p>
</div>
<div id="main-effects-ale-plots" class="section level3">
<h3>Main effects: ALE plots</h3>
<p>Next, we want to display <strong>accumulated local effects (ALE) plots</strong> as our preferred way of visualizing main effects of features in our black-box model <span class="citation">(<a href="#ref-molnar.2019.iml.book" role="doc-biblioref">Molnar 2019</a>)</span>:</p>
<pre class="r"><code>top3_terrain_vars &lt;- stringr::str_subset(
  imp$results$feature, &quot;Gabor&quot;, negate = TRUE)[1:3]
wfeatures &lt;- c(&quot;Gabor1&quot;, &quot;Gabor2&quot;, &quot;Gabor3&quot;,
               top3_terrain_vars)</code></pre>
<pre class="r"><code>wpredictor &lt;- Predictor$new(
  wfit, data = warp(d, warper = wrp),
  y = yvar, type = &quot;prob&quot;, class = &quot;TRUE&quot;
)
weffs &lt;- FeatureEffects$new(wpredictor, features = wfeatures)</code></pre>
<pre class="r"><code>plot(weffs)</code></pre>
<p><img src="figuresstrucpcaaleplot-1.png" width="80%" style="display: block; margin: auto;" /></p>
<p>This is much tidier than the 46 ALE plots shown earlier, and domain experts are perfectly able to explain this to an educated audience.</p>
<p>What about interactions? In untransformed feature space, it would be absolutely impossible to look at all the <span class="math inline">\(p(p-1)/2\)</span> pairwise interactions. In transformed PC space, in contrast, we can focus on the first PCs, and here I will show only the interaction of the first two PCs:</p>
<pre class="r"><code>weffs_2d_pdp &lt;- FeatureEffect$new(
    wpredictor, feature = c(&quot;Gabor1&quot;, &quot;Gabor2&quot;),
    method = &quot;pdp&quot;, grid.size = c(20, 20))</code></pre>
<pre class="r"><code>plot(weffs_2d_pdp)</code></pre>
<p><img src="figuresstrucpcapdp2plot-1.png" width="50%" style="display: block; margin: auto;" /></p>
</div>
<div id="references" class="section level3">
<h3>References</h3>
<div id="refs" class="references csl-bib-body hanging-indent">
<div id="ref-brenning.2021.wiml" class="csl-entry">
Brenning, A. 2021. <span>“Transforming Feature Space to Interpret Machine Learning Models.”</span> <a href="http://arxiv.org/abs/2104.04295">http://arxiv.org/abs/2104.04295</a>.
</div>
<div id="ref-brenning.et.al.2012.gabor" class="csl-entry">
Brenning, A., S. Long, and P. Fieguth. 2012. <span>“Detecting Rock Glacier Flow Structures Using <span>Gabor</span> Filters and <span>IKONOS</span> Imagery.”</span> <em>Remote Sensing of Environment</em> 125: 227–37. <a href="https://doi.org/10.1016/j.rse.2012.07.005">https://doi.org/10.1016/j.rse.2012.07.005</a>.
</div>
<div id="ref-molnar.2019.iml.book" class="csl-entry">
Molnar, C. 2019. <em>Interpretable Machine Learning: A Guide for Making Black Box Models Explainable</em>.
</div>
<div id="ref-murdoch.et.al.2019.iml" class="csl-entry">
Murdoch, W. J., C. Singh, K. Kumbier, R. Abbasi-Asl, and B. Yu. 2019. <span>“Definitions, Methods, and Applications in Interpretable Machine Learning.”</span> <em>Proceedings of the National Academy of Sciences</em> 116 (44): 22071–80. <a href="https://doi.org/10.1073/pnas.1900654116">https://doi.org/10.1073/pnas.1900654116</a>.
</div>
</div>
<p><img src="https://vg09.met.vgwort.de/na/20fa3b7a9cb04e0ab747a40747dd81c5" width="1" height="1" alt="" /></p>
</div>
</div>
